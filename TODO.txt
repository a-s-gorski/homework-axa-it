- finish pipeline and code fixing
- review code
- add .yml
- write integration and e2e tests
- write comprehensive readme.md
- clean up pyproject.toml - divide the dependencies etc
- write docs



You said:
ok help me write sphinx based markdown documentation
i need main page describing the pipeline
and subpages for each component with documentation how to use config files that i will provide you as well
help me write documentation files but only after I send you all of the code to document










training_data: type: mlstream.datasets.rda_dataset.RdaDataset object_name: pg15training output_directory: data/02_intermediate/ filepath: data/01_raw/pg15training.rda version: null converted_training_data: type: pandas.CSVDataset filepath: data/02_intermediate/pg15training.csv processed_training_data: type: pandas.CSVDataset filepath: data/03_primary/processed_training_data.csv X_train: type: pandas.CSVDataset filepath: data/04_model_inputs/X_train.csv y_train: type: pandas.CSVDataset filepath: data/04_model_inputs/y_train.csv X_val: type: pandas.CSVDataset filepath: data/04_model_inputs/X_val.csv y_val: type: pandas.CSVDataset filepath: data/04_model_inputs/y_val.csv model: type: pickle.PickleDataset filepath: data/05_training/model.pkl versioned: true training_results: type: json.JSONDataset filepath: data/05_training/training_results.json versioned: true y_pred: type: pandas.CSVDataset filepath: data/06_model_output/y_pred.csv versioned: false training_metrics_json: type: json.JSONDataset filepath: data/07_reporting/training_metrics.json versioned: false save_args: indent: 2 ensure_ascii: false confusion_matrix_fig: type: matplotlib.MatplotlibWriter filepath: data/07_reporting/confusion_matrix.png save_args: dpi: 150 bbox_inches: tight preprocess: target: # source: Column in the raw DataFrame to derive the target from. source: Numtppd # name: Name of the target column to create. name: target # strategy: How to create/validate the target. # - binary: produce 0/1 labels. # - multiclass: # TODO - not yet implemented # - regression: numeric target (processor coerces to float; fill invalids if configured) strategy: binary # rule (binary only): How to turn source values into {0,1}. # - nonzero: 1 if value != 0 else 0 # - threshold: 1 if value > threshold else 0 # - mapping: look up value in mapping (unmapped values can fall back) rule: nonzero # threshold: Used only when rule: threshold # threshold: 0.0 # mapping: Used only when rule: mapping. Keys are raw values, values are 0/1 labels. # mapping: # OK: 0 # WARN: 1 # FAIL: 1 # invalid_target_replacement: Optional fallback for invalid/unmapped values (binary/regression). # invalid_target_replacement: 0 # drop_columns: Columns to remove AFTER target creation # (the processor will never drop the final target even if listed here). drop_columns: - Numtppd - Numtpbi - Indtppd - Indtpbi # categorical_columns: Columns to one-hot encode via get_dummies. categorical_columns: - CalYear - Gender - Type - Category - Occupation - SubGroup2 - Group2 - Group1 # fillna: Per-column NA imputation BEFORE encoding/target creation. # fillna: # Occupation: "UNK" # Bonus: 0 # Age: 0 # features: If provided, keep ONLY these columns plus the target at the end. # Useful to align the design matrix with a training recipe. # features: # - engines # - passenger_capacity # - crew # - d_check_complete # - moon_clearance_complete # - iata_approved # - company_rating # - review_scores_rating # Example: binary by threshold # target: # source: claim_amount # name: is_large_claim # strategy: binary # rule: threshold # threshold: 1000.0 # Example: binary by mapping # target: # source: status # name: is_negative # strategy: binary # rule: mapping # mapping: # OK: 0 # WARN: 1 # FAIL: 1 # invalid_target_replacement: 0 # fallback for unseen labels # Example: regression target with NA fallback # target: # source: LossAmount # name: target # strategy: regression # # invalid_target_replacement: 0.0 model_options: target_column: 'target' test_size: 0.3 random_state: 42 stratify: True modeling: # Which model to instantiate. Your _get_model_class() should map names like: # lightgbm -> lightgbm.LGBMClassifier # random_forest -> sklearn.ensemble.RandomForestClassifier # logreg -> sklearn.linear_model.LogisticRegression model: type: lightgbm params: objective: binary n_estimators: 120 learning_rate: 0.05 max_depth: 6 num_leaves: 31 min_child_samples: 20 subsample: 0.8 colsample_bytree: 0.8 n_jobs: -1 verbosity: -1 # Metric used for evaluation inside search (if enabled). # Your node currently supports "accuracy". Extend in code if needed. metric: accuracy # Search block is present but disabled. The node will train directly using model.params. search: enabled: false # <-- direct training (no Optuna) n_trials: 20 # ignored when enabled=false direction: maximize # ignored when enabled=false test_size: 0.2 # inner validation split during search (ignored when enabled=false) space: {} # ignored when enabled=false # ====================================================================== # ALTERNATIVE (COMMENTED): Enable Optuna hyperparameter search instead # Toggle by commenting OUT the active modeling above and un-commenting # everything below. # ====================================================================== # modeling: # model: # type: lightgbm # # Base params act as defaults; suggested params from search will override these. # params: # objective: binary # n_jobs: -1 # verbosity: -1 # # metric: accuracy # # search: # enabled: true # n_trials: 20 # direction: maximize # test_size: 0.2 # inner split for validation inside the Optuna objective # # Define the search space. Supported types in code: int | float | categorical # space: # n_estimators: { type: int, low: 50, high: 300 } # learning_rate: { type: float, low: 0.001, high: 0.3, log: true } # max_depth: { type: int, low: 3, high: 10 } # num_leaves: { type: int, low: 20, high: 150 } # min_child_samples: { type: int, low: 5, high: 100 } # subsample: { type: float, low: 0.5, high: 1.0 } # colsample_bytree: { type: float, low: 0.5, high: 1.0 } # # # Example alternative model (Random Forest): # # modeling: # # model: # # type: random_forest # # params: # # n_estimators: 300 # # max_depth: 12 # # min_samples_split: 2 # # min_samples_leaf: 1 # # n_jobs: -1 # # metric: accuracy # # search: # # enabled: true # # n_trials: 25 # # direction: maximize # # test_size: 0.2 # # space: # # n_estimators: { type: int, low: 100, high: 600 } # # max_depth: { type: int, low: 4, high: 20 } # # min_samples_split: { type: int, low: 2, high: 20 } # # min_samples_leaf: { type: int, low: 1, high: 10 } # # # Example logistic regression (no search) # # modeling: # # model: # # type: logreg # # params: # # solver: liblinear # # max_iter: 200 # # C: 1.0 # # metric: accuracy # # search: # # enabled: false cm_labels: null # e.g. [0, 1] or ["neg", "pos"] cm_normalize: null # one of: null, "true", "pred", "all"















from __future__ import annotations import logging from typing import Any import pandas as pd from mlstream.pipelines.data_processing.dataframe_processor import ( DataFrameProcessor, DataFrameProcessorConfig, TargetStrategy, ) logger = logging.getLogger(__name__) def preprocess_dataframe(df: pd.DataFrame, parameters: dict[str, Any]) -> pd.DataFrame: """ Kedro node that preprocesses a DataFrame using DataFrameProcessor. Assumes **all** necessary options are passed via parameters. Expected parameters structure (keys are required unless marked optional): target: source: str # column to derive target from name: str # resulting target column name strategy: str # "binary" | "multiclass" | "regression" # binary-only (optional): rule: str # "nonzero" | "threshold" | "mapping" threshold: float # when rule == "threshold" mapping: dict # when rule == "mapping" drop_columns: list[str] # columns to drop categorical_columns: list[str]# columns to one-hot encode # optional: fillna: dict[str, Any] # per-column NA fills invalid_target_replacement: int | float | str features: list[str] # if provided, keep only these + target Returns: pd.DataFrame: processed DataFrame. """ if not isinstance(parameters, dict): raise TypeError( "parameters must be a dict loaded by Kedro (e.g., params:preprocess)." ) tgt = parameters["target"] strategy = TargetStrategy(tgt["strategy"]) # "binary" | "multiclass" | "regression" config = DataFrameProcessorConfig( columns_to_drop=list(parameters.get("drop_columns", [])), columns_to_onehotencode=list(parameters.get("categorical_columns", [])), columns_to_fillna=dict(parameters.get("fillna", {})), target_source=tgt["source"], target_name=tgt["name"], target_strategy=strategy, binary_rule=tgt.get("rule"), binary_threshold=tgt.get("threshold"), binary_mapping=tgt.get("mapping", {}), invalid_target_replacement=parameters.get("invalid_target_replacement"), ) processor = DataFrameProcessor(config) df_proc = processor.process(df) features: list[str] | None = parameters.get("features") if features: keep = set(features) | {config.target_name} existing = [c for c in df_proc.columns if c in keep] if existing: if ( config.target_name not in existing and config.target_name in df_proc.columns ): existing.append(config.target_name) df_proc = df_proc[existing] return df_proc from kedro.pipeline import Node, Pipeline from .nodes import preprocess_dataframe def create_pipeline(**kwargs) -> Pipeline: return Pipeline( [ Node( func=preprocess_dataframe, inputs=["training_data", "params:preprocess"], outputs="processed_training_data", name="preprocess_dataframe_node", ) ] ) from kedro.pipeline import Node, Pipeline from .nodes import split_train_test, train_or_search_model def create_pipeline(**kwargs) -> Pipeline: return Pipeline( [ Node( func=split_train_test, inputs=["processed_training_data", "params:model_options"], outputs=["X_train", "X_val", "y_train", "y_val"], name="split_data_node", ), Node( func=train_or_search_model, inputs=["X_train", "y_train", "X_val", "y_val", "params:modeling"], name="train_or_search_model_node", outputs=["model", "training_results"], ), ] ) from typing import Any import pandas as pd from sklearn.model_selection import train_test_split from .models import retrieve_model_class as _get_model_class from .optuna_train import handle_optuna_search def split_train_test( df: pd.DataFrame, parameters: dict[str, Any], ) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: """ Kedro node: split features/target into train/test using config. Inputs: - df: The processed dataset (must contain the target column). - parameters (from params:data_science), expected keys: target_column: str # e.g., "target" test_size: float # e.g., 0.2 random_state: int # e.g., 42 stratify: bool (optional, default True) # if True, stratify by target Outputs: - X_train, X_test, y_train, y_test """ target_col: str = parameters["target_column"] test_size: float = float(parameters["test_size"]) random_state: int = int(parameters["random_state"]) stratify_enabled: bool = bool(parameters.get("stratify", True)) if target_col not in df.columns: raise KeyError(f"Target column '{target_col}' not found in DataFrame.") X = df.drop(columns=[target_col]) y = df[target_col] stratify = y if stratify_enabled else None X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=test_size, random_state=random_state, stratify=stratify ) return X_train, X_val, y_train, y_val def train_or_search_model( X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame, y_val: pd.Series, parameters_modeling: dict[str, Any], ) -> tuple[Any, dict[str, Any]]: """ Kedro node: either train a model directly or run Optuna hyperparameter search. Inputs: - X_train, y_train - parameters_modeling (from params:modeling), expected structure: model: type: lightgbm | random_forest | logreg | xgboost params: {...} # base params for direct training or as defaults in search search: enabled: bool n_trials: int direction: maximize | minimize test_size: float # validation split size (e.g., 0.2) space: # per-model search space (see examples below) n_estimators: {type: int, low: 10, high: 200} learning_rate: {type: float, low: 0.001, high: 0.3, log: true} ... metric: accuracy # (currently supports 'accuracy') - parameters_ds (from params:data_science), expected keys: random_state: int # reused for the inner validation split during search Outputs: - fitted model - info dict (e.g., {"best_params": {...}, "score": float}) """ model_cfg = parameters_modeling["model"] model_name: str = model_cfg["type"] base_params: dict[str, Any] = dict(model_cfg.get("params", {})) metric_name: str = parameters_modeling.get("metric", "accuracy") search_cfg: dict[str, Any] = parameters_modeling.get("search", {"enabled": False}) model_class = _get_model_class(model_name) if not search_cfg.get("enabled", False): model = model_class(**base_params) model.fit(X_train, y_train) return model, {"best_params": base_params, "score": None, "mode": "direct"} best_params, best_value, n_trials = handle_optuna_search( search_cfg, X_train, y_train, X_val, y_val, base_params, model_class, model_name, metric_name, ) model = model_class(**best_params) model.fit(X_train, y_train) return model, { "best_params": best_params, "best_value": best_value, "n_trials": n_trials, "mode": "optuna", } """ Module: evaluation ------------------ Provides helper functions for model evaluation and visualization of classification results. Includes: - Generating predictions aligned to test data indices. - Computing common classification metrics (accuracy, precision, recall, F1). - Creating confusion matrix plots with optional normalization. Dependencies ------------ - pandas - scikit-learn - matplotlib Functions --------- predict_labels(best_model, X_test) Generate predicted class labels as a pandas Series aligned with test data. metrics_json(y_true, y_pred) Compute accuracy, precision, recall, and F1 score as a serializable dictionary. confusion_matrix_figure(y_true, y_pred, labels=None, normalize=None) Plot and return a confusion matrix as a Matplotlib Figure. Usage example ------------- >>> from sklearn.ensemble import RandomForestClassifier >>> from mypkg.evaluation import predict_labels, metrics_json, confusion_matrix_figure >>> model = RandomForestClassifier().fit(X_train, y_train) >>> preds = predict_labels(model, X_test) >>> metrics = metrics_json(y_test, preds) >>> fig = confusion_matrix_figure(y_test, preds, normalize="true") >>> fig.show() """ from collections.abc import Sequence from typing import Optional import matplotlib.pyplot as plt import pandas as pd from sklearn.metrics import ( ConfusionMatrixDisplay, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, ) def predict_labels(best_model, X_test) -> pd.Series: """ Generate class predictions as a pandas Series aligned to the index of X_test. Parameters ---------- best_model : object A trained classifier with a .predict() method. X_test : array-like or pandas.DataFrame or pandas.Series Test feature data for which predictions are made. Returns ------- pandas.Series Predicted class labels, aligned to X_test's index (if available). The name is set to <X_test.name>_pred when X_test is a named Series. Examples -------- >>> preds = predict_labels(model, X_test) >>> preds.head() 0 1 1 0 dtype: int64 """ preds = best_model.predict(X_test) index = getattr(X_test, "index", None) s = pd.Series(preds, index=index) if isinstance(X_test, pd.Series) and X_test.name: s.name = f"{X_test.name}_pred" return s def metrics_json(y_true, y_pred) -> dict[str, float]: """ Compute basic classification metrics and return them as a JSON-serializable dict. Parameters ---------- y_true : array-like Ground truth target labels. y_pred : array-like Predicted class labels. Returns ------- dict[str, float] Dictionary containing: - "accuracy": Accuracy score. - "precision": Precision score. - "recall": Recall score. - "f1": F1 score. Notes ----- Division-by-zero cases are handled gracefully with zero_division=0. Examples -------- >>> metrics_json([0, 1, 1, 0], [0, 1, 0, 0]) {'accuracy': 0.75, 'precision': 1.0, 'recall': 0.5, 'f1': 0.67} """ return { "accuracy": float(accuracy_score(y_true, y_pred)), "precision": float(precision_score(y_true, y_pred, zero_division=0)), "recall": float(recall_score(y_true, y_pred, zero_division=0)), "f1": float(f1_score(y_true, y_pred, zero_division=0)), } def confusion_matrix_figure( y_true, y_pred, labels: Optional[Sequence] = None, normalize: Optional[str] = None, ): """ Create a confusion matrix visualization as a Matplotlib Figure. Parameters ---------- y_true : array-like Ground truth (correct) target labels. y_pred : array-like Predicted target labels. labels : sequence, optional List of label names for display order in the confusion matrix. normalize : {'true', 'pred', 'all'}, optional Normalization mode for confusion matrix. Pass None for raw counts. Returns ------- matplotlib.figure.Figure Matplotlib Figure object containing the confusion matrix plot. Examples -------- >>> fig = confusion_matrix_figure(y_test, y_pred, normalize='true') >>> fig.show() """ cm = confusion_matrix(y_true, y_pred, labels=labels, normalize=normalize) fig = plt.figure() ax = fig.add_subplot(111) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot(ax=ax, colorbar=True) ax.set_title( "Confusion Matrix" + (f" (normalize={normalize})" if normalize else "") ) fig.tight_layout() return fig from kedro.pipeline import Node, Pipeline from .nodes import confusion_matrix_figure, metrics_json, predict_labels def create_pipeline(**kwargs): return Pipeline( [ Node( func=predict_labels, inputs=["model", "X_val"], outputs="y_pred", name="predict_labels_node", ), Node( func=metrics_json, inputs=["y_val", "y_pred"], outputs="training_metrics_json", name="metrics_json_node", ), Node( func=confusion_matrix_figure, inputs={ "y_true": "y_val", "y_pred": "y_pred", "labels": "params:cm_labels", "normalize": "params:cm_normalize", }, outputs="confusion_matrix_fig", name="confusion_matrix_fig_node", ), ] )